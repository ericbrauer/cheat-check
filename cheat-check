#!/usr/bin/env python3

from fileinput import filename
from re import sub, match, findall
import sys, os, glob, fnmatch
# import pycode_similar as pysim
import shutil
import subprocess
import argparse
import ast, traceback
from difflib import SequenceMatcher as sm
from keyword import iskeyword

'''
November 2021: trying to improve this. pycode_similar not giving good results. Attempt is: difflib.Sequencematcher
November 2023: more TODOs
TODO ability to run a diff directly on both files
    create a .config file where executable is stored
    finally do packaging.
TODO migrate viewers into a different module
TODO revisit the idea of improved diff checking by stripping out all non-keywords
TODO performance!
'''


user_list = []
test_set = set()

def get_args():
    "invoke argparse, passes to obj in global scope"
    parser = argparse.ArgumentParser(description="checks for submission similarity, plaigiarism",epilog="Copyright 2020 - Eric Brauer")
    parser.add_argument("-p", "--pattern", default='*.py', help="specify wildcard pattern for files to check. Ex: 'a2_*.py'")
    parser.add_argument("-x", "--exclude", default='*check*.py', help="specify wildcard pattern for files to ignore, like check scripts. Ex: 'checkA2.py'")
    parser.add_argument("-t", "--threshold", type=float, default='80', help="When to spawn alert, in percent similarity. Default is 80.")
    parser.add_argument("-b", "--bytes", type=int, help="sets a minimum byte size for comparisons. Use this to filter out assignments with no new code.")
    parser.add_argument("-f", "--target-file", help="when you want to compare all other instances to a target file. Can improve performance.")
    parser.add_argument("-s", "--static-output", action="store_true", help="Outputs results to standard output, as opposed to a menu for analysis. Menu not yet implemented.")
    parser.add_argument("target_directory", nargs='?', help="directory to find student code")
    args = parser.parse_args() 
    if args.target_directory == None:
        args.target_directory = os.getcwd()
    if not os.path.isdir(args.target_directory):
        print("Please enter a valid file path.")
        sys.exit(1)
    return args

def get_file_list(args):
    "this is an alternate method to get applicable files"
    target_dir = args.target_directory
    pattern = args.pattern
    exclude = args.exclude
    new_list = list()
    for root, dirs, files in os.walk(target_dir):
        for file in files:
            if not fnmatch.fnmatch(file, pattern):
                continue
            elif fnmatch.fnmatch(file, exclude):
                continue
            else:
                path = os.path.join(root, file)
                new_list.append(path)
    return new_list

def compare_two_files(f1, f2):
    "new for nov 2021: file comparison using difflib approach"
    if f1 != f2:
        with open(f1) as file1:
            c1 = filter_comments_from_py(file1)
        with open(f2) as file2:
            c2 = filter_comments_from_py(file2)
        m = sm(None, c1, c2)
        return m.ratio()
    return 0

def new_ratio(c1: list, c2: list) -> float:
    "compare two files, return a value of % in common"
    total = max(len(c1), len(c2))  # the longest file in number of lines creates total
    c1s = set(c1)
    c2s = set(c2)
    x = 0
    if len(c1s) >= len(c2s):
        largest = c1s
        smallest = c2s
    else:
        largest = c2s
        smallest = c1s
    for line in largest:
        if line in smallest:
            x += 1
    result = x / total
    return result


def strip_nonkw(c1: str) -> str:
    "strips out all var names"
    wordreg = r'\w+'
    lst = findall(wordreg, c1)
    raw = c1
    for word in lst:
        if not iskeyword(word):
            raw = sub(word, '', raw, count=1)
    return raw

def file_contains_errors(file_to_test):
    "second attempt to check for errors"
    print(file_to_test)
    with open(file_to_test) as f:
        source = f.read()
    valid = True
    try:
        ast.parse(source)
    except (SyntaxError, IndentationError):
        valid = False
        traceback.print_exc()  # Remove to silence any errros
    except:
        valid = False
    return valid 

def get_unique_relationships(filename_list):
    "from a list, get all unique one-to-one relationships between files"
    outer = set()  # using sets to filter duplicates
    for target in filename_list:
        for candidate in filename_list:
            if target == candidate:  # don't match file with itself
                pass
            else:
                inner = frozenset((target, candidate))  # these are hashable, allow nesting
                outer.add(inner)
    return outer

def get_target_relationships(filename_list, target):
    "when specifying a target, get all unique one-to-one relationships to it"
    outer = set()  # using sets to filter duplicates
    for candidate in filename_list:
        if target == candidate:  # don't match file with itself
            pass
        else:
            inner = frozenset((target, candidate))  # these are hashable, allow nesting
            outer.add(inner)
    return outer

def calculate_scores(outer_set):
    "take our uniques, call_pysim, return dict. key is the filenames, value is pcnt"
    rtrn_dict = {}  # {(f1, f2):99, etc.}
    for filenames in outer_set:
        filenames = tuple(filenames)  # needs to hashable
        pcnt = compare_two_files(filenames[0], filenames[1])
        if pcnt == None:
            pcnt = 0
        rtrn_dict[filenames] = float(pcnt)
    return rtrn_dict

def parse_filename(fname):
    "break filename into tuple"
    '''
    This would have worked only with blackboard submission, but not with github submissions. so you can probably removed it.
    '''
    bname = os.path.basename(fname)
    _, file = os.path.split(fname)
    try:
        lst = file.split('_')
        task = lst[0]
        stname = lst[1]
        labfile = lst[-1]
        if stname == labfile:
            stname = ""
    except (ValueError, IndexError):
        task = ""
        stname = ""
        labfile = file
    return (bname, task, stname, labfile)

def filter_comments_from_py(file_obj):
    "given a file object, remove docstrings and inline comments"
    docreg = r"[\'\"]{3}(.|\n)+?[\'\"]{3}"  # filter between """ or '''
    comreg = r"#(\w|\s)*\n"  # filter # to end of line
    raw = file_obj.read()
    raw = sub(docreg, '', raw)
    raw = sub(comreg, '\n', raw) 
    return raw


def print_dict_results(rdict, threshold):
    "all that hard work pays off"
    global args
    width = shutil.get_terminal_size(fallback=(160, 24)).columns  # not using this, print relative paths instead
    middle = 20
    lr = int((width - middle) / 2)
    for index, w in enumerate(sorted(rdict, key=rdict.get, reverse=True), start=1):  # sort the dict by value (pcnt), hi to lo
        if rdict[w] >= threshold:
            common = os.path.commonpath([w[0], w[1]])
            lb = os.path.relpath(w[0], start=common)
            _, lt, ls, lfile = parse_filename(w[0]) # lb/rb are returning complete filename
            if ls == "":  # if filename parsing fails,
                left = f"{lb}"[lr * -1:]  # print just the filename
            else:
                left = f"{ls}: {lfile}"[lr * -1:]  # lr*-1: cuts off start
            rb = os.path.relpath(w[1], start=common)
            _, rt, rs, rfile = parse_filename(w[1])
            if rs == "":
                right = f"{rb}"[lr * -1:]
            else:
                right = f"{rs}: {rfile}"[lr * -1:]

            midstring = rdict[w]
            print(f"{index:>3}. {left : >30}\t{midstring : ^.1%}\t{right : <30}")
    
    
def filter_out_smalls(input, bytes):
    "if a filesize is less than what's specified, remove from set"
    for file in input:
        if os.path.getsize(file) <= bytes:
            input.remove(file)
    return input

if __name__ == '__main__':
    args = get_args()
    threshold = args.threshold
    if 1.0 <= threshold < 100:  # if user gives pcnt in range 0-100
        threshold /= 100  # change to 0.0-1.0
    print(f"Testing with a threshold of {threshold : .1%}.")
    test_list = get_file_list(args)
    if len(test_list) <= 1:
        print('No files to analyze.')
        exit(1)
    errorlist = []  # this will store the files excluded b/c non-zero exit codes
    if args.bytes:
        test_list = filter_out_smalls(test_list, args.bytes)
    print('Files to test: ' + str(len(test_list)))
    if args.target_file:
        uniques = get_target_relationships(test_list, args.target_file)
    else:
        uniques = get_unique_relationships(test_list)
    results = calculate_scores(uniques)
    print_dict_results(results, threshold)
